---
import BaseLayout from '../layouts/BaseLayout.astro';
---

<BaseLayout title="Research — Shruti Joshi">
  <h1 class="text-2xl font-semibold text-zinc-900 dark:text-zinc-50 tracking-tight mb-8">
    Research
  </h1>

  <div class="space-y-12">
    <!-- Area 1 -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-accent-100 dark:bg-accent-900/50 flex items-center justify-center text-accent-600 dark:text-accent-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 3v11.25A2.25 2.25 0 006 16.5h2.25M3.75 3h-1.5m1.5 0h16.5m0 0h1.5m-1.5 0v11.25A2.25 2.25 0 0118 16.5h-2.25m-7.5 0h7.5m-7.5 0l-1 3m8.5-3l1 3m0 0l.5 1.5m-.5-1.5h-9.5m0 0l-.5 1.5" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          Identifiable Representation Learning
        </h2>
      </div>
      <p class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl">
        When can we provably recover the latent factors that generated observed
        data? I study the structural conditions—dependence, redundancy,
        dimensionality mismatch—under which identifiability guarantees hold
        and, critically, under which the metrics used to evaluate them break
        down. My recent work introduces a taxonomy of metric misspecification
        and shows that standard scores (MCC, DCI, etc.) produce systematic
        false positives and negatives under common structural departures.
      </p>
      <div class="mt-3 flex flex-wrap gap-2">
        <span class="tag">disentanglement</span>
        <span class="tag">nonlinear ICA</span>
        <span class="tag">causal representation learning</span>
        <span class="tag">evaluation</span>
      </div>
    </section>

    <!-- Area 2 -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-emerald-100 dark:bg-emerald-900/50 flex items-center justify-center text-emerald-600 dark:text-emerald-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M7.5 21L3 16.5m0 0L7.5 12M3 16.5h13.5m0-13.5L21 7.5m0 0L16.5 12M21 7.5H7.5" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          Causal Inference for Interpretability
        </h2>
      </div>
      <p class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl">
        Mechanistic interpretability makes causal claims—that a circuit
        implements a function, that a feature encodes a concept—yet often
        relies on evidence that supports only weaker, associational
        conclusions. I use Pearl's causal hierarchy to formalise what
        interpretability methods can justify and show how causal
        representation learning provides the right bridge between
        observation and intervention on model internals.
      </p>
      <div class="mt-3 flex flex-wrap gap-2">
        <span class="tag">causal inference</span>
        <span class="tag">mechanistic interpretability</span>
        <span class="tag">LLMs</span>
        <span class="tag">activation patching</span>
      </div>
    </section>

    <!-- Area 3 -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-violet-100 dark:bg-violet-900/50 flex items-center justify-center text-violet-600 dark:text-violet-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M9.813 15.904L9 18.75l-.813-2.846a4.5 4.5 0 00-3.09-3.09L2.25 12l2.846-.813a4.5 4.5 0 003.09-3.09L9 5.25l.813 2.846a4.5 4.5 0 003.09 3.09L15.75 12l-2.846.813a4.5 4.5 0 00-3.09 3.09z" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          Representation Geometry of Pretrained Models
        </h2>
      </div>
      <p class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl">
        Modern pretrained models (LLMs, vision transformers) produce
        overcomplete, distributed representations whose geometry is poorly
        understood. I study how semantic structure is allocated across
        coordinates, when probing results reflect genuine model computation
        versus artefacts of basis choice, and how to evaluate representation
        quality when no ground-truth factors are available.
      </p>
      <div class="mt-3 flex flex-wrap gap-2">
        <span class="tag">superposition</span>
        <span class="tag">probing</span>
        <span class="tag">linear representations</span>
        <span class="tag">LLMs</span>
      </div>
    </section>
  </div>

  <!-- Open questions -->
  <section class="mt-16 pt-8 border-t border-zinc-100 dark:border-zinc-800/50">
    <h2 class="section-heading">Open Questions I Think About</h2>
    <ul class="space-y-3 text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed">
      <li class="flex gap-3">
        <span class="shrink-0 text-accent-400">→</span>
        When can identifiability results be empirically verified, and when must
        we accept that the metric itself is the weakest link?
      </li>
      <li class="flex gap-3">
        <span class="shrink-0 text-accent-400">→</span>
        What is the right notion of "concept" for evaluating LLM representations
        when ground-truth factors are unavailable?
      </li>
      <li class="flex gap-3">
        <span class="shrink-0 text-accent-400">→</span>
        Can we design metrics that are simultaneously robust to overcompleteness,
        correlation, and basis choice?
      </li>
    </ul>
  </section>
</BaseLayout>
