---
import BaseLayout from '../layouts/BaseLayout.astro';
---

<BaseLayout title="Research â€” Shruti Joshi">
  <h1 class="text-2xl font-semibold text-zinc-900 dark:text-zinc-50 tracking-tight mb-10">
    Research
  </h1>

  <!-- Brief orienting paragraph -->
  <p class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl mb-12">
    My research asks when learned representations provably recover semantically meaningful structure, and the corresponding practical payoff for interpretability and control. Below are brief teasers of the conceptual space around some of my recent and upcoming work.
  </p>

  <div class="space-y-14">

    <!-- 1. Identifiability + Interpretability -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-purple-100 dark:bg-purple-900/50 flex items-center justify-center text-purple-600 dark:text-purple-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 3v11.25A2.25 2.25 0 006 16.5h2.25M3.75 3h-1.5m1.5 0h16.5m0 0h1.5m-1.5 0v11.25A2.25 2.25 0 0118 16.5h-2.25m-7.5 0h7.5m-7.5 0l-1 3m8.5-3l1 3m0 0l.5 1.5m-.5-1.5h-9.5m0 0l-.5 1.5" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
        Causality and Identifiability for Generalisable Interpretability and Control
        </h2>
      </div>
      <div class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl space-y-3">
        <p>
          Pretrained models are powerful but difficult to adapt minimally to downstream tasks or to steer for counterfactual control. Identifiable representation learning provides a potentially useful mathematical foundation for both: by guaranteeing uniqueness of latent variables under appropriate assumptions, it specifies exactly what structure a representation contains, what can be intervened on independently, and what cannot.
        </p>
        <p>
          Interpretability is thus the first application: if the recovered variables are provably unique, causal claims about circuits and features become testable. My first project develops the <strong class="text-zinc-700 dark:text-zinc-300">Sparse Shift Autoencoder (SSAE)</strong>, which recovers identifiable concept vectors from LLM representations by imposing sparsity not on the concepts themselves, but on the distribution of concept shifts across paired observations. This posits concepts naturally as factors varying in observed data, and does not require access to any privileged pairs; it works by uniformly sampling any two prompts. We validate SSAEs on multiple LLMs and leverage constrained optimisation for stable sparse optimisation across scale and choice of data.
        </p>
        <p>
          Two co-authored studies probe where existing tools break. The first builds a multi-factor benchmark to study if concepts recovered by sparse autoencoders are isolated in single dimensions (they are not!). The second separates the linear representation hypothesis from linear separability and shows that amortised unsupervised featurisers degrade systematically under distribution shift. Both failures are structural and both point to the need for evaluation methods that account for concept geometry and identifiability rather than assuming them.
        </p>
      </div>
    </section>

    <!-- 2. Optimisation as a Bottleneck for Operationalising Identifiability-->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-amber-100 dark:bg-amber-900/50 flex items-center justify-center text-amber-600 dark:text-amber-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M3 13.125C3 12.504 3.504 12 4.125 12h2.25c.621 0 1.125.504 1.125 1.125v6.75C7.5 20.496 6.996 21 6.375 21h-2.25A1.125 1.125 0 013 19.875v-6.75zM9.75 8.625c0-.621.504-1.125 1.125-1.125h2.25c.621 0 1.125.504 1.125 1.125v11.25c0 .621-.504 1.125-1.125 1.125h-2.25a1.125 1.125 0 01-1.125-1.125V8.625zM16.5 4.125c0-.621.504-1.125 1.125-1.125h2.25C20.496 3 21 3.504 21 4.125v15.75c0 .621-.504 1.125-1.125 1.125h-2.25a1.125 1.125 0 01-1.125-1.125V4.125z" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          Optimisation as a Bottleneck for Operationalising Identifiability
        </h2>
      </div>
      <div class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl space-y-3">
        <p>
          Achieving identifiability in practice in unsupervised representation learning is also constrained by optimisation bottlenecks. If we assume the quality of a representation to be indicated by its degree of identifiability (and not predictive loss as in supervised learnig), we need to study its variations in an analogous manner to how generalisation error is studied, while leveraging the benefits of over-parameterisation, which is uncommon in identifiable representation learning. In a limited set of experiments, I have observed that constrained optimisation helps, ensuring stable hyperparameter transfer. The open problem is characterising how parameterisation, architecture, and optimiser bias jointly determine reachability of identifiable solutions, and deriving scaling laws for identifiability error analogous to those for predictive loss.
        </p>
      </div>
    </section>

    <!-- 3. Evaluating Identifiability on Real-World Data -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-emerald-100 dark:bg-emerald-900/50 flex items-center justify-center text-emerald-600 dark:text-emerald-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M9 12.75L11.25 15 15 9.75m-3-7.036A11.959 11.959 0 013.598 6 11.99 11.99 0 003 9.749c0 5.592 3.824 10.29 9 11.623 5.176-1.332 9-6.03 9-11.622 0-1.31-.21-2.571-.598-3.751h-.152c-3.196 0-6.1-1.248-8.25-3.285z" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          Evaluating Identifiability on Real-World Data through Causal Hypothesis Testing
        </h2>
      </div>
      <div class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl space-y-3">
        <p>
          Widely used evaluation metrics for assessing identifiability conflate properties of the data distribution with properties of the encoding function, and therefore fail to assess the structural quality of a representation. The core difficulty is that these metrics rest on restrictive assumptions about what a good representation looks like. In practice, useful representations may distribute a single factor across multiple latent dimensions. They also fail when the ground-truth factors are themselves strongly correlated, precisely the regime where theory still guarantees recoverability but empirical evaluation becomes misleading. Worse, in realistic domains where ground truth is unavailable (such as in interpretability research), these metrics implicitly assume the learned representation must match a fixed "true" latent dimensionality, even though this quantity is neither observable nor well-defined. Models that benefit from over-parameterisation are penalised simply because they violate assumptions hard-coded into the metric. My research reframes evaluation through testable implications aligned with the hierarchy of questions a representation should support. The approach is to construct controlled task families that assess what the representation knows, without requiring access to unobserved ground truth.
        </p>
      </div>
    </section>

    <!-- 4. Applications -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-rose-100 dark:bg-rose-900/50 flex items-center justify-center text-rose-600 dark:text-rose-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M9.813 15.904L9 18.75l-.813-2.846a4.5 4.5 0 00-3.09-3.09L2.25 12l2.846-.813a4.5 4.5 0 003.09-3.09L9 5.25l.813 2.846a4.5 4.5 0 003.09 3.09L15.75 12l-2.846.813a4.5 4.5 0 00-3.09 3.09z" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          Applications: Jailbreak Mechanisms and Multi-Agent Cooperation
        </h2>
      </div>
      <div class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl space-y-3">
        <p>
          Identifiability determines whether you can reliably steer a model. I apply this framework to two concrete problems.
        </p>
        <p>
          <strong class="text-zinc-700 dark:text-zinc-300">Jailbreak mechanisms.</strong>
          LLMs remain vulnerable to jailbreaks, but little is known about the internal mechanisms that differentiate safe from adversarial behaviour. The key observation is that while jailbreaks are mediated by dense perturbations in input text space, the corresponding changes in latent space should be relatively sparse, making them amenable to identifiable decomposition (such as via SSAEs). Where is such safety-relevant information stored in the network (non-localised) to be reliably controlled at the appropriate level of abstraction?
        </p>
        <p>
          <strong class="text-zinc-700 dark:text-zinc-300">Multi-agent cooperation.</strong>
          When LLMs act as interacting agents in social dilemmas, do they internalise behavioural strategies such as cooperation, defection, reciprocity? Do LLMs encode identifiable behavioural vectors that generalise across model families and dialogue contexts that can be manipulated to control their behaviour in multi-agent settings?
        </p>
      </div>
    </section>

  </div>
</BaseLayout>