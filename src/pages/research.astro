---
import BaseLayout from '../layouts/BaseLayout.astro';
---

<BaseLayout title="Research — Shruti Joshi">
  <h1 class="text-2xl font-semibold text-zinc-900 dark:text-zinc-50 tracking-tight mb-10">
    Research
  </h1>

  <!-- Brief orienting paragraph -->
  <p class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl mb-12">
    My research asks when learned representations provably recover the structure they need, how to tell when they have, and what goes wrong — in theory and in practice — when they haven't. The work spans identifiability theory, optimisation, evaluation methodology, and applications to LLM safety and multi-agent behaviour.
  </p>

  <div class="space-y-14">

    <!-- 1. Identifiability + Interpretability -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-purple-100 dark:bg-purple-900/50 flex items-center justify-center text-purple-600 dark:text-purple-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 3v11.25A2.25 2.25 0 006 16.5h2.25M3.75 3h-1.5m1.5 0h16.5m0 0h1.5m-1.5 0v11.25A2.25 2.25 0 0118 16.5h-2.25m-7.5 0h7.5m-7.5 0l-1 3m8.5-3l1 3m0 0l.5 1.5m-.5-1.5h-9.5m0 0l-.5 1.5" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          Identifiability as a Foundation for Interpretability
        </h2>
      </div>
      <div class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl space-y-3">
        <p>
          Modern pretrained models learn representations that are empirically powerful but structurally opaque: we cannot say which latent variables they have recovered, whether those variables are unique, or what class of questions they license — associational, interventional, or counterfactual. Identifiable representation learning provides the mathematical foundation for answering these questions by guaranteeing uniqueness of latent variables under appropriate assumptions.
        </p>
        <p>
          My first project develops the <strong class="text-zinc-700 dark:text-zinc-300">Sparse Shift Autoencoder (SSAE)</strong>, which recovers identifiable concept vectors from LLM representations by imposing sparsity not on the concepts themselves, but on the distribution of concept shifts across paired observations. The key insight is that for a broad class of representations — including those learned by modern LLMs — this shift-sparsity constraint is sufficient for identifiability. We validate on multiple LLMs and leverage constrained optimisation for stable, scalable recovery of sparse structure.
        </p>
        <p>
          In co-authored work, we probe the limits of existing interpretability tools from two directions: a multi-factor benchmark where sentiment, domain, and tense co-vary shows that disentanglement methods fail outright when concepts are correlated; a separate study distinguishes the linear representation hypothesis from linear separability and reveals systematic failures of unsupervised featurisers under distribution shift. Together, these results expose geometric and abstraction-level breakdowns in prevailing interpretability techniques, and motivate approaches that explicitly reason about concept geometry, causal structure, and identifiability.
        </p>
      </div>
    </section>

    <!-- 2. Optimisation -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-amber-100 dark:bg-amber-900/50 flex items-center justify-center text-amber-600 dark:text-amber-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M3 13.125C3 12.504 3.504 12 4.125 12h2.25c.621 0 1.125.504 1.125 1.125v6.75C7.5 20.496 6.996 21 6.375 21h-2.25A1.125 1.125 0 013 19.875v-6.75zM9.75 8.625c0-.621.504-1.125 1.125-1.125h2.25c.621 0 1.125.504 1.125 1.125v11.25c0 .621-.504 1.125-1.125 1.125h-2.25a1.125 1.125 0 01-1.125-1.125V8.625zM16.5 4.125c0-.621.504-1.125 1.125-1.125h2.25C20.496 3 21 3.504 21 4.125v15.75c0 .621-.504 1.125-1.125 1.125h-2.25a1.125 1.125 0 01-1.125-1.125V4.125z" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          When Optimisation Fails to Find Identifiable Solutions
        </h2>
      </div>
      <div class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl space-y-3">
        <p>
          Identifiability is a property of the statistical model. Whether training actually reaches an identifiable solution is a question about optimisation — and the answer is frequently no. A central finding from my experiments is that even when identifiability is theoretically guaranteed, gradient-based training fails to reach the identifiable solution basin because the optimisation trajectory never realises the sufficient conditions required for recovery. The effective hypothesis class becomes restricted in ways that prevent convergence to the correct solution, while an over-parameterised but improperly constrained system can achieve low predictive loss on training data yet produce representations that are geometrically wrong and fail under distribution shift.
        </p>
        <p>
          This problem sits outside the scope of traditional statistical learning theory, which analyses generalisation error and statistical risk but not the structural quality of the learned representation — here understood through its degree of identifiability. I aim to characterise how architectural design, deliberate over-parameterisation, and the implicit biases of optimisation algorithms interact to determine whether identifiable solutions are accessible to training dynamics. This includes deriving empirical scaling laws for identifiability error as a function of dataset size, parameter count, and optimisation trajectory — analogous to classical scaling laws, but measuring recoverability of latent structure rather than predictive loss.
        </p>
      </div>
    </section>

    <!-- 3. Evaluation -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-emerald-100 dark:bg-emerald-900/50 flex items-center justify-center text-emerald-600 dark:text-emerald-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M9 12.75L11.25 15 15 9.75m-3-7.036A11.959 11.959 0 013.598 6 11.99 11.99 0 003 9.749c0 5.592 3.824 10.29 9 11.623 5.176-1.332 9-6.03 9-11.622 0-1.31-.21-2.571-.598-3.751h-.152c-3.196 0-6.1-1.248-8.25-3.285z" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          Evaluation as Hypothesis Testing
        </h2>
      </div>
      <div class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl space-y-3">
        <p>
          Widely used evaluation metrics — MCC, DCI, R² — conflate properties of the data distribution with properties of the learner, and therefore fail to assess the structural quality of a representation. The core difficulty is that these metrics rest on restrictive assumptions about what a good representation looks like. In practice, useful representations injectively encode the underlying factors through nonlinear transformations, or distribute a single factor across multiple latent dimensions — correlation-based metrics cannot faithfully evaluate either case. They also fail when the ground-truth factors are themselves strongly correlated, precisely the regime where theory still guarantees recoverability but empirical evaluation becomes misleading. Worse, in realistic domains where ground truth is unavailable, these metrics implicitly assume the learned representation must match a fixed "true" latent dimensionality, even though this quantity is neither observable nor well-defined. Models that benefit from over-parameterisation — a pattern central to modern deep learning — are penalised simply because they violate assumptions hard-coded into the metric.
        </p>
        <p>
          My research reframes evaluation through testable implications aligned with the hierarchy of questions a representation should support. At the associational level: metrics invariant to invertible transformations and sensitive to informational sufficiency rather than coordinate alignment. At the interventional level: whether learned latents support stable, compositional interventions. At the counterfactual level: whether abstracted latent variables support coherent hypothetical reasoning in the presence of correlated or redundant factors. The approach is to construct controlled task families that assess what the representation knows, without requiring access to unobserved ground truth.
        </p>
      </div>
    </section>

    <!-- 4. Applications -->
    <section>
      <div class="flex items-center gap-3 mb-3">
        <div class="w-8 h-8 rounded-lg bg-rose-100 dark:bg-rose-900/50 flex items-center justify-center text-rose-600 dark:text-rose-400">
          <svg class="w-4 h-4" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M9.813 15.904L9 18.75l-.813-2.846a4.5 4.5 0 00-3.09-3.09L2.25 12l2.846-.813a4.5 4.5 0 003.09-3.09L9 5.25l.813 2.846a4.5 4.5 0 003.09 3.09L15.75 12l-2.846.813a4.5 4.5 0 00-3.09 3.09z" />
          </svg>
        </div>
        <h2 class="text-lg font-semibold text-zinc-900 dark:text-zinc-100">
          Applications: Jailbreak Mechanisms and Multi-Agent Cooperation
        </h2>
      </div>
      <div class="text-sm text-zinc-600 dark:text-zinc-400 leading-relaxed max-w-2xl space-y-3">
        <p>
          Identifiability determines whether you can reliably steer a model. I apply this framework to two concrete problems.
        </p>
        <p>
          <strong class="text-zinc-700 dark:text-zinc-300">Jailbreak mechanisms.</strong>
          LLMs remain vulnerable to jailbreaks, but little is known about the internal mechanisms that differentiate safe from adversarial behaviour. The key observation is that while jailbreaks are mediated by dense perturbations in input text space, the corresponding changes in latent space should be relatively sparse — making them amenable to identifiable decomposition via SSAEs. A specific question is where in the network safety-relevant information is represented versus discarded: because next-token prediction must preserve grammatical and discourse coherence, safety-related structure may be suppressed in early layers while coherence-related structure propagates to later ones. We contrast embeddings across token positions and layers, and evaluate whether recovered features are genuinely identifiable through controlled task families rather than post-hoc matching to human-interpretable labels.
        </p>
        <p>
          <strong class="text-zinc-700 dark:text-zinc-300">Multi-agent cooperation.</strong>
          When LLMs are deployed as interacting agents, success depends on whether they internalise behavioural strategies or mimic surface patterns. Using in-context imitation learning as a diagnostic — supplying multi-turn demonstrations of expert policies in text-based social dilemmas such as the iterated Prisoner's Dilemma — we generate trajectory families converging on cooperative versus defective behaviour, feed them to a monitor LLM, and contrast hidden-state embeddings to extract latent shifts corresponding to the underlying strategies. The central question is whether LLMs encode identifiable behavioural vectors that generalise across model families, episodes, and dialogue contexts, and whether these vectors can be used to steer cooperation at test time.
        </p>
      </div>
    </section>

  </div>
</BaseLayout>